{
  "articles": [
    {
      "path": "classifier_bert.html",
      "title": "캡스톤 프로젝트",
      "description": "TBD\n",
      "author": [
        {
          "name": "이은정",
          "url": "https://2222Eunjeong.github.io/"
        },
        {
          "name": "홍길동",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n유사도 정렬 수집\ncreate UDF\n\n–\n\n* 검색된 총 기사 건수는 2740778건입니다.\n- (100/1000)건 호출을 진행합니다.\n- (200/1000)건 호출을 진행합니다.\n- (300/1000)건 호출을 진행합니다.\n- (400/1000)건 호출을 진행합니다.\n- (500/1000)건 호출을 진행합니다.\n- (600/1000)건 호출을 진행합니다.\n- (700/1000)건 호출을 진행합니다.\n- (800/1000)건 호출을 진행합니다.\n- (900/1000)건 호출을 진행합니다.\n- (1000/1000)건 호출을 진행합니다.\n\n유사도 정렬 수집\nnews_worldcup_sim <- search_naver(\nkeyword, client_id = client_id, client_secret = client_secret, sort = “sim”,\ndo_done = TRUE, max_record = n\n)\ndim(news_worldcup_date)\ndim(news_worldcup_sim)\nhead(news_worldcup_date)\ntail(news_worldcup_sim)\ncreate UDF\ncreate_wordcloud <- function(data, remove_n = 5, min_freq = 5, background = “white”) {\ndata %>%\nfilter(nchar(description_text) > 0) %>%\ntidytext::unnest_tokens(noun, description_text, bitTA::morpho_mecab, type = “noun”) %>%\ngroup_by(noun) %>%\ncount() %>%\narrange(desc(n)) %>%\nungroup() %>%\nfilter(n >= min_freq) %>%\nfilter(row_number() > remove_n) %>%\nwordcloud2::wordcloud2(backgroundColor = background,\nfontFamily = “NanumSquare”)\n}\nlibrary(bitReport)\nnews_worldcup_date %>%\ncreate_wordcloud(remove_n = 20, min_freq = 2)\nnews_worldcup_sim %>%\ncreate_wordcloud(remove_n = 20, min_freq = 2)\npersons <- c(“벤투”, “손흥민”, “조규성”, “이강인”, “호날두”, “메시”)\npersons %>%\npurrr::map_int(\nfunction(x) {\nnews_worldcup_sim %>%\nfilter(stringr::str_detect(description_text, x)) %>%\ntally() %>%\npull()\n}\n)\npersons <- c(“벤투”, “손흥민”, “조규성”, “이강인”, “호날두”, “메시”)\npersons %>%\npurrr::map_dbl(\nfunction(x) {\nnews_worldcup_sim %>%\nfilter(stringr::str_detect(description_text, x)) %>%\nmutate(n_talk = stringr::str_count(description_text, x)) %>%\nsummarise(n_avg = mean(n_talk, na.rm = TRUE)) %>%\npull()\n}\n)\nnews_worldcup_sim <- news_worldcup_sim %>%\nmutate(id = row_number())\nlibrary(tidyverse)\nlibrary(bitTA)\nlibrary(tidytext)\nlibrary(tm)\ndtm_tf <- news_worldcup_sim %>%\nunnest_noun_ngrams(term, description_text, n = 1, type = “noun2”) %>%\nfilter(!str_detect(term, “[[a-zA-Z]]+”)) %>%\ncount(id, term, sort = TRUE) %>%\ncast_dtm(id, term, n)\ntm::inspect(dtm_tf)\ndtm_tfidf <- news_worldcup_sim %>%\nunnest_noun_ngrams(term, description_text, n = 1, type = “noun2”) %>%\nfilter(!str_detect(term, “[[a-zA-Z]]+”)) %>%\ncount(id, term, sort = TRUE) %>%\ncast_dtm(id, term, n, weighting = tm::weightTfIdf)\ntm::inspect(dtm_tfidf)\npersons <- c(“벤투”, “손흥민”, “조규성”, “이강인”, “호날두”, “메시”)\npersons %>%\npurrr::map(\nfunction(x) tm::findAssocs(dtm_tf, terms = x, corlimit = 0.4)\n)\npersons <- c(“벤투”, “손흥민”, “조규성”, “이강인”, “호날두”, “메시”)\npersons %>%\npurrr::map(\nfunction(x) tm::findAssocs(dtm_tfidf, terms = x, corlimit = 0.4)\n)\ndtm_bin_tf <- news_worldcup_sim %>%\nunnest_noun_ngrams(term, description_text, n = 1, type = “noun2”) %>%\nfilter(!str_detect(term, “[[a-zA-Z]]+”)) %>%\ncount(id, term, sort = TRUE) %>%\ncast_dtm(id, term, n, weighting = tm::weightBin)\nstop_words <- dtm_bin_tf %>%\napply(2, sum) %>%\nsort(decreasing = TRUE) %>%\n“[”(1:30) %>%\nnames()\nstop_words\ndtm_bin_tf <- news_worldcup_sim %>%\nunnest_noun_ngrams(term, description_text, n = 1, type = “noun2”) %>%\nfilter(!term %in% stop_words) %>%\nfilter(!str_detect(term, “[[a-zA-Z]]+|[[0-9]]+”)) %>%\ncount(id, term, sort = TRUE) %>%\ncast_dtm(id, term, n, weighting = tm::weightBin)\nlibrary(“arules”)\ntrans <- as(dtm_bin_tf %>% as.matrix(), “transactions”)\ntrans\nsummary(trans)\nrules <- apriori(trans, parameter = list(support = 0.05, conf = 0.6, target = “rules”))\nsummary(rules)\narules::inspect(rules[1:5])\nlibrary(“arulesViz”)\nplot(rules)\nrule2 <- sort(rules, by = “confidence”)\ninspect(head(rule2, n = 10))\nplot(rules, method = “grouped”)\nplot(rules, method = “graph”)\ndim(dtm_bin_tf)\ncompact_bin <- tm::removeSparseTerms(dtm_bin_tf, sparse = 0.985) %>%\nas.matrix(compact_bin)\ndim(compact_bin)\nmat <- t(compact_bin)\ndist_matrix <- dist(scale(mat))\nfit <- hclust(dist_matrix, method = “ward.D”)\nfit\nk <- 6\nplot(fit)\ncluster_list <- rect.hclust(fit, k = k)\nk %>%\nseq() %>%\npurrr::map(\nfunction(x) {\ncluster_list[[x]]\n}\n)\nmat <- compact_bin\ndist_matrix <- dist(scale(mat))\nfit <- hclust(dist_matrix, method = “ward.D”)\nfit\n\n\n\n",
      "last_modified": "2022-12-10T15:15:17+09:00"
    },
    {
      "path": "classifier_lasso.html",
      "title": "대통령 연설문 예측",
      "description": "TBD\n",
      "author": [
        {
          "name": "이은정",
          "url": "https://2222Eunjeong.github.io/"
        },
        {
          "name": "홍길동",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n웹페이지 기술내용\n준비하기\n\nFrequency 기반의 DTM 생성\ntokenize 반복기 정의\n\n띄어쓰기 단위로 토큰을 생성\n\n\n웹페이지 기술내용\n준비하기\n패키지 로드하기\n\n\nlibrary(tidymodels)\nlibrary(bitTA)\nlibrary(text2vec)\n\n\n데이터셋 분리\n\n\nset.seed(123)\npresident_split <- rsample::initial_split(president_speech, prop = 7/8,\n    strata = president)\n\npresident_smpl <- rsample::testing(president_split)\n\nset.seed(123)\npresident_split <- initial_split(president_smpl, prop = 0.7, strata = president)\n\ntrain <- rsample::training(president_split)\ntest <- rsample::testing(president_split)\n\n\n\nFrequency 기반의 DTM 생성\ntokenize 반복기 정의\n띄어쓰기 단위로 토큰을 생성\n\n\ntoken_fun <- text2vec::word_tokenizer\n\nit_train <- itoken(train$doc, tokenizer = token_fun, ids = train$id, progressbar = FALSE)\n\nit_test <- itoken(test$doc, tokenizer = token_fun, ids = test$id, progressbar = FALSE)\n\n### Vocabulary 생성\n\n### Document Term Matrix 생성하기\n\npresident_speech %>%\n    group_by(category) %>%\n    tally() %>%\n    arrange(desc(n))\n\n# A tibble: 10 × 2\n   category           n\n   <chr>          <int>\n 1 외교-통상        678\n 2 문화-체육-관광   387\n 3 정치-사회        344\n 4 국정전반         308\n 5 산업-경제        300\n 6 국방             139\n 7 교육              80\n 8 과학기술-정보     75\n 9 기타              72\n10 환경              25\n\n\n\n\n",
      "last_modified": "2022-12-10T15:15:24+09:00"
    },
    {
      "path": "create_website.html",
      "title": "웹 사이트 개발하기",
      "description": "\"웹 사이트를 개발하는 방법을 간단히 소개합니다.\"\n",
      "author": [
        {
          "name": "이은정",
          "url": "https://2222Eunjeong.github.io/"
        },
        {
          "name": "홍길동",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n설정사항\n수정해야할 사항\n웹 사이트 구성 파일\n개별 페이지 구성 정보\n\nData: USArrests\n표(tables) 출력\n플롯(plots) 출력\n\n\n\n\n\n\n들어가기\n이 웹 사이트는 예제를 위해서 만든 간단한 사이트입니다.\n\n여러분은 이 Skelton 사이트에 살을 붙여서 자신의 웹 사이트를 만들 수 있습니다. 그리고 이 작업은 사이트의 구조를 이해하는 것으로부터 시작됩니다.\n\n\n\n설정사항\n수정해야할 사항\n본 템플리트는 웹 사이트 중의 한 페이지로 bitReport website라는 이름의 예제입니다. 환경 설정파일인 _site.yml에 “샘플 웹 사이트”이라는 제목으로 연결되어 있습니다. 만약에 예제 템플리트를 완성하려면 이 페이지의 이름을 _site.yml에서의 create_website과 동일하게 설정해야 합니다.\n웹 사이트 구성 파일\n웹 사이트를 구성하는 설정은 구성파일인 **_site.yml**에 정의합니다.\n_site.yml 파일에서의 사용자가 설정해야할 항목은 다음과 같습니다.\nname: 웹 사이트의 이름\n헤더의 네비게이션 바의 왼쪽에 링크표시됩니다.\n\ntitle: 웹 사이트의 타이틀\n헤더의 네비게이션 바의 왼쪽에 링크표시됩니다.\n\ndescription: 웹 사이트의 설명\noutput_dir: 생성될 웹 사이트의 정적 HTML이 저장될 디렉토리\n“docs”로 기본설정됩니다. 이 디렉토리는 github page로 deploy할 때 유용합니다.\n\nnavbar: 웹 사이트의 메뉴를 정의하는 섹션입니다.\n수정하지 않습니다.\n\nright: 웹 사이트의 메뉴를 정의합니다.\ntext는 메뉴 이름입니다.\nhref는 메뉴와 연결할 웹 페이지입니다. 확장자는 html입니다.\nR markdown 파일과 동일하게 이름을 부여합니다.\n\nmenu는 서브메뉴를 정의합니다.\n빈 분리자를 만들기 위해서는 “- text:”—“를 사용합니다.\n\noutput: 웹 사이트 출력에 대한 설정입니다. 사용자가 수정하지 않습니다.\n개별 페이지 구성 정보\n개별 페이지를 구성하기 위해서는 knitr YAML을 수정해야 합니다.\ntitle: 웹 페이지 제목입니다.\ndescription: 웹 페이지를 간단하게 소개하는 소개문입니다.\nauthor: 웹 페이지 컨텐츠 저작자 정보를 기술합니다.\nname: 저작자 이름\nurl: 저작자 개인 홈페이지 URL\naffiliation: 저작자 소속 회사/부서\naffiliation_url: 저작자 소속 회사/부서 홈페이지 URL\n\ndate: 컨텐츠를 생성한 날짜\noutput: 웹 사이트 출력에 대한 설정입니다.\ntoc: 목차를 출력할 지의 여부를 정의합니다. true이면 출력합니다.\ntoc_depth: 출력할 목차의 depth를 정의합니다. 3이면 3 depth까지 표시합니다.\n\n\n이 예제 웹 사이트는 하나의 완성된 페이지를 만드는 것이 아닌, 가상의 site를 담은 Skelton만 제공합니다. 그러므로 개별 페이지의 내용에 신경쓸 필요가 없습니다.\n\nData: USArrests\nUSArrests는 미국 주별 강력 범죄율을 기록한 데이터입니다.\n이 데이터셋은 4개의 변수와 50개의 관측치로 구성된 데이터 프레임(data.frame) 객체입니다.:\nMurder\nnumeric. 살인범 검거 건수(100,000건당)\n\nAssault\nnumeric. 폭행범 검거 건수(100,000건당)\n\nUrbanPop\nnumeric. 도시 인구 비율(백분율)\n\nRape\nnumeric. 강간범 검거 건수(100,000건당)\n\n\n\n# code here\n\n\n표(tables) 출력\n미국 주별 강력 범죄율을 기록한 데이터인 USArrests를 표로 출력합니다.\n\n\nUSArrests %>%\n    tibble::rownames_to_column(\"주 (State)\") %>%\n    arrange(desc(Murder + Assault + Rape)) %>%\n    filter(row_number() <= 10) %>%\n    select(1:3, 5, 4) %>%\n    rename(살인범 = Murder) %>%\n    rename(폭행범 = Assault) %>%\n    rename(강간범 = Rape) %>%\n    rename(`도시인구수(백분율)` = UrbanPop) %>%\n    kableExtra::kbl(caption = \"미국 범죄 상위 10개 주 현황\",\n        format.args = list(big.mark = \",\", digits = 1, scientific = 6)) %>%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>%\n    kableExtra::add_header_above(c(` ` = 1, `범죄자수 (인구 만명 당)` = 3,\n        ` ` = 1)) %>%\n    kableExtra::kable_classic(full_width = TRUE)\n\n\nTable 1: 미국 범죄 상위 10개 주 현황\n\n\n\n\n\n범죄자수 (인구 만명 당)\n\n\n\n\n\n주 (State)\n\n\n살인범\n\n\n폭행범\n\n\n강간범\n\n\n도시인구수(백분율)\n\n\nFlorida\n\n\n15\n\n\n335\n\n\n32\n\n\n80\n\n\nNorth Carolina\n\n\n13\n\n\n337\n\n\n16\n\n\n45\n\n\nMaryland\n\n\n11\n\n\n300\n\n\n28\n\n\n67\n\n\nArizona\n\n\n8\n\n\n294\n\n\n31\n\n\n80\n\n\nNew Mexico\n\n\n11\n\n\n285\n\n\n32\n\n\n70\n\n\nCalifornia\n\n\n9\n\n\n276\n\n\n41\n\n\n91\n\n\nAlaska\n\n\n10\n\n\n263\n\n\n44\n\n\n48\n\n\nSouth Carolina\n\n\n14\n\n\n279\n\n\n22\n\n\n48\n\n\nNevada\n\n\n12\n\n\n252\n\n\n46\n\n\n81\n\n\nMichigan\n\n\n12\n\n\n255\n\n\n35\n\n\n74\n\n\n플롯(plots) 출력\n이 예제는 가상의 설명을 포함하고 있는, 그저 템플리트를 위한 예제입니다.\n온도에 따른 수은의 증기압을 기록한 데이터인 pressure 데이터 프레임을 산점도록 시각화합니다.\n\n\nplot(pressure, pch = 16, main = \"Relation between temperature and pressure\")\nlines(loess(pressure ~ temperature, pressure), col = \"steelblue\")\n\n\n\nFigure 1: 플롯 예제\n\n\n\n\n\n\n",
      "last_modified": "2022-12-10T15:15:27+09:00"
    },
    {
      "path": "index.html",
      "title": "텍스트 데이터 분석",
      "description": "내가 생각하는 텍스트 분석은 긴 문장 속에서 핵심을 찾아내는 것. 어떤게 중요한 키워드인지 찾아내는 과정. 회사 상담이력 데이터에서 주요 키워드를 찾아보고 싶습니다.\n",
      "author": [
        {
          "name": "이은정",
          "url": "https://2222Eunjeong.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n내가 기대하는 텍스트 분석\n\n내가 기대하는 텍스트 분석\n음성 인식(Speech Recognition)이란 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리를 말합니다. STT(Speech-to-Text)라고도 합니다.\n상담센터의 상담 내용을 텍스트화한 STT 데이터를 자연어 처리(NLP, Natural Language Processing)하여 함축되어 있는 고객의 의도를 이해합니다.\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-12-10T15:15:28+09:00"
    }
  ],
  "collections": []
}
